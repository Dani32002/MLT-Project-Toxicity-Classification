# -*- coding: utf-8 -*-
"""hate_speech_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yHs6y5riy5Cq6NJdGfmWqOHHlwnFZ81-

# **Machine Learning Techniques Project**


*   Nicolas Bedoya Figueroa
*   Daniel Escalante Perez
*   Marilyn Stephany Joven Fonseca
*   Eder Leandro Carbonero Baquero

## **Utils**
"""

!pip install nltk pyspellchecker tqdm emoji nlpaug transformers

#Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
from spellchecker import SpellChecker
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from tqdm import tqdm
import nlpaug.augmenter.word as naw
from nlpaug.util import Action
import emoji
import random
import math
from nltk.corpus import wordnet
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger_eng')

"""## **Data preprocessing**

### **Dataset 1: Davidson et al. 2017**
"""

# Load the dataset
davidson = pd.read_csv("./data/davidson_2017.csv")[["class", "tweet"]]
davidson.head()

# Original class labels: 0 - hate speech, 1 - offensive language, 2 - neither
# Transform the label in 1: toxic and 0: non toxic
davidson["class"] = davidson["class"].replace({0: 1, 2: 0})
davidson["class"].value_counts()

print("Rows:", davidson.shape[0])
print("Columns:", davidson.shape[1])

"""### **Dataset 2: HASOC (2019) English**"""

hasoc = pd.read_csv("./data/HASOC_EN.tsv",sep = '\t')[["text","task_1"]]
hasoc.head()

# Transform the label to 1: toxic and 0: non toxic
hasoc["task_1"] = hasoc["task_1"].replace({"HOF": 1, "NOT": 0})
hasoc["task_1"].value_counts()

# Change column names to match the other datasets
hasoc = hasoc.rename(columns={'task_1': 'class', 'text': 'tweet'})
hasoc.head()

print("Rows:", hasoc.shape[0])
print("Columns:", hasoc.shape[1])

"""### **Dataset 3: Zeerak Talatâ€™s Hate Speech Dataset**"""

# Load the dataset
zeerak = pd.read_csv("./data/NAACL_SRW_2016_fixed.csv")[["class", "text"]]
zeerak.head()

# Original class labels
zeerak["class"].value_counts()

# Transform the label to 1: toxic and 0: non toxic
zeerak["class"] = zeerak["class"].replace({"sexism": 1, "racism": 1, "none": 0})
zeerak["class"].value_counts()

# Rename the columns to match the other datasets
zeerak = zeerak.rename(columns={'text': 'tweet'})
zeerak.head()

print("Rows:", zeerak.shape[0])
print("Columns:", zeerak.shape[1])

"""### **Concatenation**"""

data = pd.concat([davidson, hasoc, zeerak], axis=0, ignore_index=True)
data.head()

data["class"].value_counts()

print("Rows:", data.shape[0])
print("Columns:", data.shape[1])

"""### **Cleaning**"""

#Check point
data_cleaning = data.copy()

# Transform emojis into words

def emoji_to_words(text):
  return emoji.demojize(text, language='en')

data_cleaning['tweet'] = data_cleaning['tweet'].apply(emoji_to_words)

# Remove URLs from tweets

def remove_urls(text):
  return re.sub(r'http\S+', '', text)

data_cleaning['tweet'] = data_cleaning['tweet'].apply(remove_urls)

# Remove mentions from tweets
def remove_mentions(text):
  return re.sub(r'@\w+', '', text)

data_cleaning['tweet'] = data_cleaning['tweet'].apply(remove_mentions)

# Remove symbols from tweets

def leave_letters(text):
  return re.sub(r'[^a-zA-Z]', ' ', text)

data_cleaning['tweet'] = data_cleaning['tweet'].apply(leave_letters)

# Remove symbols from tweets

def lowercase(text):
  return text.lower()

data_cleaning['tweet'] = data_cleaning['tweet'].apply(lowercase)

# Correct spelling
spell = SpellChecker()

def correct_spelling(text):
  words = text.split()
  corrected_words = [spell.correction(word) or word for word in words]
  return ' '.join(corrected_words)

data_cleaning['tweet'] = [correct_spelling(text) for text in tqdm(data_cleaning['tweet'])]

# Remove stopwords

def remove_stopwords(text):
  stop_words = set(stopwords.words('english'))  # Use English stop words
  words = text.split()
  filtered_words = [word for word in words if word not in stop_words]
  return " ".join(filtered_words)

data_cleaning['tweet'] = data_cleaning['tweet'].apply(remove_stopwords)

# Stemming the words

stemmer = PorterStemmer()

def stem_text(text):
  words = text.split()
  stemmed_words = [stemmer.stem(word) for word in words]
  return " ".join(stemmed_words)

  data_cleaning['tweet'] = data_cleaning['tweet'].apply(stem_text)

data_cleaning['tweet'].head(200)

# Check and remove duplicates
print(f'Duplicates: {data_cleaning["tweet"].duplicated().sum()}')
duplicated_tweets = data_cleaning["tweet"].duplicated()
data_cleaning = data_cleaning[~duplicated_tweets]

# Check for null or empty again if they appeared due to augmentation

keep = ~((data_cleaning["tweet"].isnull()) | (data_cleaning["tweet"] == ""))

print(f'Number of nulls or empty: {(~keep).sum()}')

data_cleaning = data_cleaning[keep]

print(f'Cleaned data shape: {data_cleaning.shape}')

# Checking the dataset's balance

data_cleaning["class"].value_counts()

# Function to get a random synonym of a word

def get_synonym(word):
  synonyms = []
  for syn in wordnet.synsets(word):
    for lemma in syn.lemmas():
      synonyms.append(lemma.name())
  if len(synonyms) > 0:
    synonyms = list(set(synonyms))
    return synonyms[random.randint(0, len(synonyms) - 1)]
  else:
    return ""

# Custom random insertion function

def random_synonym_insert_augment(text, alpha):

  words = text.split()
  new_text = words.copy()
  for word in words:
    if random.random() < alpha:
      synonym = get_synonym(word)
      if synonym != "":
        position = random.randint(0, len(new_text) - 1)
        new_text.insert(position, synonym)

  return [" ".join(new_text)]

# Balance the data set using easy data augmentation

#Choose an alpha parameter (Percentage of words in a sentence that are changed)
alpha = 0.25

aug_synonym = naw.SynonymAug(aug_src='wordnet', aug_p = alpha)
aug_swap = naw.RandomWordAug(action="swap", aug_p = alpha)
aug_delete = naw.RandomWordAug(action="delete", aug_p = alpha)


# Store the new rows
new_rows = []

# Size to balance the classes
desired_size = len(data_cleaning[data_cleaning['class'] == 1]) - len(data_cleaning[data_cleaning['class'] == 0])

non_toxic = data_cleaning[data_cleaning['class'] == 0]

# Until balanced
while len(new_rows) < desired_size:
  # Get a random sample from the minority class
  random_row = non_toxic.sample(1)

  # Pick a random EDA technique and apply it
  random_num = random.randint(1, 4)
  augmented_text = []

  if random_num == 1:
    augmented_text = aug_synonym.augment(random_row['tweet'].values[0])
  elif random_num == 2:
    augmented_text = aug_swap.augment(random_row['tweet'].values[0])
  elif random_num == 3:
    augmented_text = random_synonym_insert_augment(random_row['tweet'].values[0], alpha)
  else:
    augmented_text = aug_delete.augment(random_row['tweet'].values[0])

  if len(augmented_text) > 0:
    new_rows.append({ "tweet": augmented_text[0], "class": random_row['class'].values[0] })


# New rows dataframe
new_rows_df = pd.DataFrame(new_rows)

# Concatenate the datasets
balanced_data = pd.concat([data_cleaning, new_rows_df], ignore_index = True, axis = 0)

print(f'Balanced data shape: {balanced_data.shape}')
print()
balanced_data["class"].value_counts()

# Check and remove duplicates again that could have appeared due to augmentation
print(f'Duplicates: {balanced_data["tweet"].duplicated().sum()}')
duplicated_tweets = balanced_data["tweet"].duplicated()
balanced_data = balanced_data[~duplicated_tweets]

# Check for null or empty again if they appeared due to augmentation

keep = ~((balanced_data["tweet"].isnull()) | (balanced_data["tweet"] == ""))

print(f'Number of nulls or empty: {(~keep).sum()}')

balanced_data = balanced_data[keep]

print(f'Balanced data shape: {balanced_data.shape}')

# Final distribution

balanced_data["class"].value_counts()

balanced_data.tail(100)

balanced_data.to_csv('balanced_data.csv', index=False)