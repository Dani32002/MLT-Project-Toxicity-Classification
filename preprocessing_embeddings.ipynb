{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGQH-ELe7b-J"
   },
   "source": [
    "# **Machine Learning Techniques Project**\n",
    "\n",
    "\n",
    "*   Nicolas Bedoya Figueroa\n",
    "*   Daniel Escalante Perez\n",
    "*   Marilyn Stephany Joven Fonseca\n",
    "*   Eder Leandro Carbonero Baquero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6o8TFLYF8aQ1"
   },
   "source": [
    "## **Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pki8KEw1Pujd",
    "outputId": "623088e3-eaac-48a4-ee4d-ce777a57c69f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: pyspellchecker in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (0.8.2)\n",
      "Requirement already satisfied: tqdm in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: emoji in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (2.14.1)\n",
      "Requirement already satisfied: nlpaug in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (1.1.11)\n",
      "Requirement already satisfied: transformers in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: tensorflow in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (2.19.0)\n",
      "Requirement already satisfied: xgboost[gpu] in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (3.0.0)\n",
      "Requirement already satisfied: accelerate in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: joblib in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: click in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from nlpaug) (2.1.3)\n",
      "Requirement already satisfied: requests>=2.22.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from nlpaug) (2.32.3)\n",
      "Requirement already satisfied: gdown>=4.0.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from nlpaug) (5.2.0)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from nlpaug) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: filelock in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: setuptools in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from tensorflow) (3.0.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from tensorflow) (4.13.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from tensorflow) (25.2.10)\n",
      "\u001b[33mWARNING: xgboost 3.0.0 does not provide the extra 'gpu'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scipy in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from xgboost[gpu]) (1.15.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from xgboost[gpu]) (2.21.5)\n",
      "Requirement already satisfied: psutil in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (4.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: rich in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: optree in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: namex in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2025.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (2.3.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: networkx in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: jinja2 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk pyspellchecker tqdm emoji nlpaug transformers tensorflow xgboost[gpu] accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: transformers in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: torch in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: tqdm in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from seaborn) (2.1.3)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from seaborn) (3.10.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: requests in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: filelock in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: networkx in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: fsspec in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch) (4.13.1)\n",
      "Requirement already satisfied: jinja2 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.57.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn transformers torch tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: umap-learn in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (0.5.7)\n",
      "Requirement already satisfied: plotly in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (6.0.1)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from umap-learn) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from umap-learn) (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from umap-learn) (2.1.3)\n",
      "Requirement already satisfied: tqdm in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from umap-learn) (4.67.1)\n",
      "Requirement already satisfied: numba>=0.51.2 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from umap-learn) (0.61.2)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: packaging in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from plotly) (24.2)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from plotly) (1.35.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn) (0.44.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/sis.virtual.uniandes.edu.co/d.escalante/venv/lib/python3.10/site-packages (from scikit-learn>=0.22->umap-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install umap-learn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "quNgOgNV8dYw",
    "outputId": "9a88919d-ff7d-4601-e95a-759126386fc3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 00:53:46.071624: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-16 00:53:46.078848: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747374826.087083    3072 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747374826.089607    3072 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747374826.096162    3072 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747374826.096171    3072 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747374826.096172    3072 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747374826.096173    3072 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-16 00:53:46.098593: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/sis.virtual.uniandes.edu.co/d.escalante/nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/sis.virtual.uniandes.edu.co/d.escalante/nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/sis.virtual.uniandes.edu.co/d.escalante/nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sis.virtual.uniandes.edu.co/d.escalante/nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/sis.virtual.uniandes.edu.co/d.escalante/nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm import tqdm\n",
    "import nlpaug.augmenter.word as naw\n",
    "from nlpaug.util import Action\n",
    "import emoji\n",
    "import random\n",
    "import math\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerias ML y DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "import umap\n",
    "import plotly.express as px\n",
    "import ast\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 611\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dI0vpkTC8LWM",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Data preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4y9SS5Pg8TjJ"
   },
   "source": [
    "### **Dataset 1: Davidson et al. 2017**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "WRllcMCL8oeV",
    "outputId": "599ff8d7-7417-44e7-fa44-42125931d1f8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                              tweet\n",
       "0      2  !!! RT @mayasolovely: As a woman you shouldn't...\n",
       "1      1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n",
       "2      1  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n",
       "3      1  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...\n",
       "4      1  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "davidson = pd.read_csv(\"./data/davidson_2017.csv\")[[\"class\", \"tweet\"]]\n",
    "davidson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "3S5c8ynlGnio",
    "outputId": "1d3f5e57-ea97-476c-f44a-5fe9aab43b3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "1    20620\n",
       "0     4163\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original class labels: 0 - hate speech, 1 - offensive language, 2 - neither\n",
    "# Transform the label in 1: toxic and 0: non toxic\n",
    "davidson[\"class\"] = davidson[\"class\"].replace({0: 1, 2: 0})\n",
    "davidson[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zctWGKDLBvlV",
    "outputId": "2585ee8d-54cc-42ae-8cc4-08310345bcc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 24783\n",
      "Columns: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Rows:\", davidson.shape[0])\n",
    "print(\"Columns:\", davidson.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtCPNKB1-PXZ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Dataset 2: HASOC (2019) English**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "il2gw4se-UGG",
    "outputId": "2b194bd1-db75-4f95-ef0d-29ad5213bfd9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>task_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#DhoniKeepsTheGlove | WATCH: Sports Minister K...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@politico No. We should remember very clearly ...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@cricketworldcup Guess who would be the winner...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Corbyn is too politically intellectual for #Bo...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All the best to #TeamIndia for another swimmin...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text task_1\n",
       "0  #DhoniKeepsTheGlove | WATCH: Sports Minister K...    NOT\n",
       "1  @politico No. We should remember very clearly ...    HOF\n",
       "2  @cricketworldcup Guess who would be the winner...    NOT\n",
       "3  Corbyn is too politically intellectual for #Bo...    NOT\n",
       "4  All the best to #TeamIndia for another swimmin...    NOT"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasoc = pd.read_csv(\"./data/HASOC_EN.tsv\",sep = '\\t')[[\"text\",\"task_1\"]]\n",
    "hasoc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "SM68qXoGHm5h",
    "outputId": "07471d31-0b6d-424e-c6c2-1a19f7bdafd7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20878/1216868155.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  hasoc[\"task_1\"] = hasoc[\"task_1\"].replace({\"HOF\": 1, \"NOT\": 0})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "task_1\n",
       "0    3591\n",
       "1    2261\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the label to 1: toxic and 0: non toxic\n",
    "hasoc[\"task_1\"] = hasoc[\"task_1\"].replace({\"HOF\": 1, \"NOT\": 0})\n",
    "hasoc[\"task_1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "370Fq7v1I8nv",
    "outputId": "99fe1a8d-b726-4cb3-9094-de6f1a61c97f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#DhoniKeepsTheGlove | WATCH: Sports Minister K...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@politico No. We should remember very clearly ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@cricketworldcup Guess who would be the winner...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Corbyn is too politically intellectual for #Bo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All the best to #TeamIndia for another swimmin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "0  #DhoniKeepsTheGlove | WATCH: Sports Minister K...      0\n",
       "1  @politico No. We should remember very clearly ...      1\n",
       "2  @cricketworldcup Guess who would be the winner...      0\n",
       "3  Corbyn is too politically intellectual for #Bo...      0\n",
       "4  All the best to #TeamIndia for another swimmin...      0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change column names to match the other datasets\n",
    "hasoc = hasoc.rename(columns={'task_1': 'class', 'text': 'tweet'})\n",
    "hasoc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rd9gFm6XByTe",
    "outputId": "e5b13e12-a649-44dd-d96e-9fc09c089c51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 5852\n",
      "Columns: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Rows:\", hasoc.shape[0])\n",
    "print(\"Columns:\", hasoc.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POVzb_McGaHw",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Dataset 3: Zeerak Talatâ€™s Hate Speech Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ZaU4jSIJGlbd",
    "outputId": "1d01550d-3cd8-4f97-c5be-4e8e43362e2a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>racism</td>\n",
       "      <td>So Drasko just said he was impressed the girls...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>racism</td>\n",
       "      <td>Drasko they didn't cook half a bird you idiot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>racism</td>\n",
       "      <td>Hopefully someone cooks Drasko in the next ep ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>racism</td>\n",
       "      <td>of course you were born in serbia...you're as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>racism</td>\n",
       "      <td>RT @YesYoureRacist: At least you're only a tin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    class                                               text\n",
       "0  racism  So Drasko just said he was impressed the girls...\n",
       "1  racism  Drasko they didn't cook half a bird you idiot ...\n",
       "2  racism  Hopefully someone cooks Drasko in the next ep ...\n",
       "3  racism  of course you were born in serbia...you're as ...\n",
       "4  racism  RT @YesYoureRacist: At least you're only a tin..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "zeerak = pd.read_csv(\"./data/NAACL_SRW_2016_fixed.csv\")[[\"class\", \"text\"]]\n",
    "zeerak.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "-JjJCvd_HOzp",
    "outputId": "3e9dd718-a82f-43d8-b8c3-b589fee38bdc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "none      7060\n",
       "sexism    2577\n",
       "racism      11\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original class labels\n",
    "zeerak[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "nphlA7FBHVwP",
    "outputId": "6a8df7a9-c8a0-43aa-ebd8-1259de47c044"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20878/990906760.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  zeerak[\"class\"] = zeerak[\"class\"].replace({\"sexism\": 1, \"racism\": 1, \"none\": 0})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    7060\n",
       "1    2588\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the label to 1: toxic and 0: non toxic\n",
    "zeerak[\"class\"] = zeerak[\"class\"].replace({\"sexism\": 1, \"racism\": 1, \"none\": 0})\n",
    "zeerak[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "DpDoOid6HvIa",
    "outputId": "c9a410e7-26ee-4114-ffbb-359bb73e76ed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>So Drasko just said he was impressed the girls...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Drasko they didn't cook half a bird you idiot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Hopefully someone cooks Drasko in the next ep ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>of course you were born in serbia...you're as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @YesYoureRacist: At least you're only a tin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                              tweet\n",
       "0      1  So Drasko just said he was impressed the girls...\n",
       "1      1  Drasko they didn't cook half a bird you idiot ...\n",
       "2      1  Hopefully someone cooks Drasko in the next ep ...\n",
       "3      1  of course you were born in serbia...you're as ...\n",
       "4      1  RT @YesYoureRacist: At least you're only a tin..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the columns to match the other datasets\n",
    "zeerak = zeerak.rename(columns={'text': 'tweet'})\n",
    "zeerak.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zM3Lo4PpH9wW",
    "outputId": "9fd02c0b-2563-441c-cb89-57346eb234ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 9648\n",
      "Columns: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Rows:\", zeerak.shape[0])\n",
    "print(\"Columns:\", zeerak.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wc6WggqZIqTd",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Concatenation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "n039VZBvIvTL",
    "outputId": "bb659b49-76c5-4362-daa6-bf189dd4915a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                              tweet\n",
       "0      0  !!! RT @mayasolovely: As a woman you shouldn't...\n",
       "1      1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n",
       "2      1  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n",
       "3      1  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...\n",
       "4      1  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([davidson, hasoc, zeerak], axis=0, ignore_index=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "3Ynv6GXmJaaC",
    "outputId": "a1b0d147-b4e2-4441-b95f-f73ef4901ad8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "1    25469\n",
       "0    14814\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LkIiH5LTJYcI",
    "outputId": "edf2ab84-60b1-4345-89d8-86142bea1477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 40283\n",
      "Columns: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Rows:\", data.shape[0])\n",
    "print(\"Columns:\", data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDCbT5h2Jpqs",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2ri0a8eqMlba"
   },
   "outputs": [],
   "source": [
    "#Check point\n",
    "# Creation of one dataset for embedding of text into a vector and another one for embbeding with GloVe + Twitter\n",
    "data_cleaning = data.copy()\n",
    "data_cleaning_slight = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "uD7dNTS-Risk"
   },
   "outputs": [],
   "source": [
    "# Transform emojis into words\n",
    "\n",
    "def emoji_to_words(text):\n",
    "  return emoji.demojize(text, language='en')\n",
    "\n",
    "data_cleaning['tweet'] = data_cleaning['tweet'].apply(emoji_to_words)\n",
    "data_cleaning_slight['tweet'] = data_cleaning_slight['tweet'].apply(emoji_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "w3W3lxHlJ0OI"
   },
   "outputs": [],
   "source": [
    "# Remove URLs from tweets\n",
    "\n",
    "def remove_urls(text):\n",
    "  return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "data_cleaning['tweet'] = data_cleaning['tweet'].apply(remove_urls)\n",
    "data_cleaning_slight['tweet'] = data_cleaning_slight['tweet'].apply(remove_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "dstAzTL0KaaX"
   },
   "outputs": [],
   "source": [
    "# Remove mentions from tweets\n",
    "def remove_mentions(text):\n",
    "  return re.sub(r'@\\w+', '', text)\n",
    "\n",
    "data_cleaning['tweet'] = data_cleaning['tweet'].apply(remove_mentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "_uF8JhSkLbft"
   },
   "outputs": [],
   "source": [
    "# Remove symbols from tweets\n",
    "\n",
    "def leave_letters(text):\n",
    "  text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "  text = re.sub(r'\\s+', ' ', text)\n",
    "  return text\n",
    "\n",
    "data_cleaning['tweet'] = data_cleaning['tweet'].apply(leave_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove uncommon symbols from tweets\n",
    "\n",
    "def clean_tweet_for_glove(text):\n",
    "  # Keep only letters, numbers, hashtags, mentions, apostrophes, emojis, and basic punctuation\n",
    "  text = re.sub(r\"[^a-zA-Z0-9@#'â€™!?.,\\s]\", '', text)  # keep @, #, apostrophes, basic punctuation\n",
    "  text = re.sub(r'\\s+', ' ', text)  # replace multiple spaces with single space\n",
    "  return text.strip()\n",
    "\n",
    "data_cleaning_slight['tweet'] = data_cleaning_slight['tweet'].apply(clean_tweet_for_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "FbJodhJyM23e"
   },
   "outputs": [],
   "source": [
    "# Remove symbols from tweets\n",
    "\n",
    "def lowercase(text):\n",
    "  return text.lower()\n",
    "\n",
    "data_cleaning['tweet'] = data_cleaning['tweet'].apply(lowercase)\n",
    "data_cleaning_slight['tweet'] = data_cleaning_slight['tweet'].apply(lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ggJB0i9qOFNG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40283/40283 [2:47:53<00:00,  4.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Correct spelling\n",
    "spell = SpellChecker()\n",
    "\n",
    "def correct_spelling(text):\n",
    "  words = text.split()\n",
    "  corrected_words = [spell.correction(word) or word for word in words]\n",
    "  return ' '.join(corrected_words)\n",
    "\n",
    "data_cleaning['tweet'] = [correct_spelling(text) for text in tqdm(data_cleaning['tweet'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "CxKYgHtCQHyK"
   },
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "  stop_words = set(stopwords.words('english'))  # Use English stop words\n",
    "  words = text.split()\n",
    "  filtered_words = [word for word in words if word not in stop_words]\n",
    "  return \" \".join(filtered_words)\n",
    "\n",
    "data_cleaning['tweet'] = data_cleaning['tweet'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "knrHot4YQ5Za"
   },
   "outputs": [],
   "source": [
    "# Stemming the words\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_text(text):\n",
    "  words = text.split()\n",
    "  stemmed_words = [stemmer.stem(word) for word in words]\n",
    "  return \" \".join(stemmed_words)\n",
    "\n",
    "  data_cleaning['tweet'] = data_cleaning['tweet'].apply(stem_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "Jo_cF0_hBOdl",
    "outputId": "3a33d01a-fee9-41ba-a8af-2066e844e10b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      woman complain cleaning house amp man always t...\n",
       "1             boy days cold toga bad coffin hoe st place\n",
       "2           dawn ever fuck bitch start cry confused shit\n",
       "3                                       look like granny\n",
       "4         shit hear might true might faker bitch told ya\n",
       "                             ...                        \n",
       "195    tired bitches saying look mean night big af we...\n",
       "196    birds grandads may never see thanks climate ch...\n",
       "197                                 stay beautiful bitch\n",
       "198    wutkinda r purple ceeeleee man girl playing st...\n",
       "199          money getting taller bitches getting blurry\n",
       "Name: tweet, Length: 200, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaning['tweet'].head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      !!! rt @mayasolovely as a woman you shouldn't ...\n",
       "1      !!!!! rt @mleew17 boy dats cold...tyga dwn bad...\n",
       "2      !!!!!!! rt @urkindofbrand dawg!!!! rt @80sbaby...\n",
       "3      !!!!!!!!! rt @cganderson @vivabased she look l...\n",
       "4      !!!!!!!!!!!!! rt @shenikaroberts the shit you ...\n",
       "                             ...                        \n",
       "195    @montrell i'm tired of bitches saying i look m...\n",
       "196    @motherjones 10 birds your grandkids may never...\n",
       "197                 @mvckfadden stay beautiful you bitch\n",
       "198    @nickiminaj #wutkinda r purple. ceeeleeeman th...\n",
       "199    @nastycopper money getting taller and bitches ...\n",
       "Name: tweet, Length: 200, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaning_slight['tweet'].head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and remove duplicates\n",
    "def check_remove_dup(dataset):\n",
    "  print(f'Duplicates: {dataset[\"tweet\"].duplicated().sum()}')\n",
    "  duplicated_tweets = dataset[\"tweet\"].duplicated()\n",
    "  return dataset[~duplicated_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XJkqwmmvIziu",
    "outputId": "0d018b93-a1fd-49f4-85f1-a152d652ee26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates: 1823\n"
     ]
    }
   ],
   "source": [
    "# Check and remove duplicates in the first dataset\n",
    "\n",
    "data_cleaning = check_remove_dup(data_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates: 185\n"
     ]
    }
   ],
   "source": [
    "# Check and remove duplicates in the second dataset\n",
    "\n",
    "data_cleaning_slight = check_remove_dup(data_cleaning_slight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null or empty \n",
    "\n",
    "def check_null_empty(dataset):\n",
    "  keep = ~((dataset[\"tweet\"].isnull()) | (dataset[\"tweet\"] == \"\"))\n",
    "    \n",
    "  print(f'Number of nulls or empty: {(~keep).sum()}')\n",
    "    \n",
    "  dataset = dataset[keep]\n",
    "    \n",
    "  print(f'Cleaned data shape: {dataset.shape}')\n",
    "\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wkX2z2k6w1VS",
    "outputId": "d3d3f856-7875-4c63-e349-e6504c55d0e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nulls or empty: 1\n",
      "Cleaned data shape: (38459, 2)\n"
     ]
    }
   ],
   "source": [
    "# Check for null or empty in the first dataset\n",
    "\n",
    "data_cleaning = check_null_empty(data_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nulls or empty: 1\n",
      "Cleaned data shape: (40097, 2)\n"
     ]
    }
   ],
   "source": [
    "# Check for null or empty in the second dataset\n",
    "\n",
    "data_cleaning_slight = check_null_empty(data_cleaning_slight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "tb2qAprnpPQJ",
    "outputId": "2164773f-0d40-43b7-b2a8-5e5c7ded19c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "1    24345\n",
       "0    14114\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the dataset's balance\n",
    "\n",
    "data_cleaning[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "1    25419\n",
       "0    14678\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the dataset's balance\n",
    "\n",
    "data_cleaning_slight[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "-e6MsX360a3r"
   },
   "outputs": [],
   "source": [
    "# Function to get a random synonym of a word\n",
    "\n",
    "def get_synonym(word):\n",
    "  synonyms = []\n",
    "  for syn in wordnet.synsets(word):\n",
    "    for lemma in syn.lemmas():\n",
    "      synonyms.append(lemma.name())\n",
    "  if len(synonyms) > 0:\n",
    "    synonyms = list(set(synonyms))\n",
    "    return synonyms[random.randint(0, len(synonyms) - 1)]\n",
    "  else:\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "8CWCzEuw0v30"
   },
   "outputs": [],
   "source": [
    "# Custom random insertion function\n",
    "\n",
    "def random_synonym_insert_augment(text, alpha):\n",
    "\n",
    "  words = text.split()\n",
    "  new_text = words.copy()\n",
    "  for word in words:\n",
    "    if random.random() < alpha:\n",
    "      synonym = get_synonym(word)\n",
    "      if synonym != \"\":\n",
    "        position = random.randint(0, len(new_text) - 1)\n",
    "        new_text.insert(position, synonym)\n",
    "\n",
    "  return [\" \".join(new_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the data set using easy data augmentation\n",
    "\n",
    "#Choose an alpha parameter (Percentage of words in a sentence that are changed)\n",
    "alpha = 0.25\n",
    "\n",
    "aug_synonym = naw.SynonymAug(aug_src='wordnet', aug_p = alpha)\n",
    "aug_swap = naw.RandomWordAug(action=\"swap\", aug_p = alpha)\n",
    "aug_delete = naw.RandomWordAug(action=\"delete\", aug_p = alpha)\n",
    "\n",
    "\n",
    "def data_augmentation(dataset, alpha):\n",
    "  # Store the new rows\n",
    "  new_rows = []\n",
    "\n",
    "  # Size to balance the classes\n",
    "  desired_size = len(dataset[dataset['class'] == 1]) - len(dataset[dataset['class'] == 0])\n",
    "\n",
    "  non_toxic = dataset[dataset['class'] == 0]\n",
    "\n",
    "  # Until balanced\n",
    "  while len(new_rows) < desired_size:\n",
    "    # Get a random sample from the minority class\n",
    "    random_row = non_toxic.sample(1)\n",
    "\n",
    "    # Pick a random EDA technique and apply it\n",
    "    random_num = random.randint(1, 4)\n",
    "    augmented_text = []\n",
    "\n",
    "    if random_num == 1:\n",
    "      augmented_text = aug_synonym.augment(random_row['tweet'].values[0])\n",
    "    elif random_num == 2:\n",
    "      augmented_text = aug_swap.augment(random_row['tweet'].values[0])\n",
    "    elif random_num == 3:\n",
    "      augmented_text = random_synonym_insert_augment(random_row['tweet'].values[0], alpha)\n",
    "    else:\n",
    "      augmented_text = aug_delete.augment(random_row['tweet'].values[0])\n",
    "\n",
    "    if len(augmented_text) > 0:\n",
    "      new_rows.append({ \"tweet\": augmented_text[0], \"class\": random_row['class'].values[0] })\n",
    "\n",
    "\n",
    "  # New rows dataframe\n",
    "  new_rows_df = pd.DataFrame(new_rows)\n",
    "\n",
    "  # Concatenate the datasets\n",
    "  return pd.concat([dataset, new_rows_df], ignore_index = True, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "yCzR8WSkm-55",
    "outputId": "fa9ea933-6dc1-4cfb-a014-2e2bfbd969d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced data shape: (48690, 2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    24345\n",
       "1    24345\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Balance the first dataset\n",
    "\n",
    "balanced_data = data_augmentation(data_cleaning, alpha)\n",
    "\n",
    "print(f'Balanced data shape: {balanced_data.shape}')\n",
    "print()\n",
    "balanced_data[\"class\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced data shape: (50838, 2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    25419\n",
       "1    25419\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Balance the second dataset\n",
    "\n",
    "balanced_data_glove = data_augmentation(data_cleaning_slight, alpha)\n",
    "\n",
    "print(f'Balanced data shape: {balanced_data_glove.shape}')\n",
    "print()\n",
    "balanced_data_glove[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VT4oS-zymJ1-",
    "outputId": "36701ecc-5424-430f-9136-7374fcdd9aa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates: 994\n"
     ]
    }
   ],
   "source": [
    "# Check and remove duplicates again that could have appeared due to augmentation\n",
    "\n",
    "balanced_data = check_remove_dup(balanced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates: 655\n"
     ]
    }
   ],
   "source": [
    "# Check and remove duplicates again that could have appeared due to augmentation in the second dataset\n",
    "\n",
    "balanced_data_glove = check_remove_dup(balanced_data_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qzl7m5dtmVb9",
    "outputId": "0a8834c2-ee10-4f87-a065-5744ea1143d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nulls or empty: 0\n",
      "Cleaned data shape: (47696, 2)\n"
     ]
    }
   ],
   "source": [
    "# Check for null or empty again if they appeared due to augmentation\n",
    "\n",
    "balanced_data = check_null_empty(balanced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nulls or empty: 0\n",
      "Cleaned data shape: (50183, 2)\n"
     ]
    }
   ],
   "source": [
    "# Check for null or empty again if they appeared due to augmentation in the second dataset\n",
    "\n",
    "balanced_data_glove = check_null_empty(balanced_data_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "yHSD5OOFnZ-0",
    "outputId": "f015c6bb-04da-480e-a83c-5e598d371777"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "1    24345\n",
       "0    23351\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final distribution\n",
    "\n",
    "balanced_data[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "1    25419\n",
       "0    24764\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final distribution\n",
    "\n",
    "balanced_data_glove[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "1uYEqJ89B_Ji"
   },
   "outputs": [],
   "source": [
    "balanced_data.to_csv('balanced_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data_glove.to_csv('balanced_data_glove.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Embeddings Secuenciales roBERTa Twitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the device to use GPU\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instanciating pre trained model\n",
    "\n",
    "model_name = \"cardiffnlp/twitter-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the stored data\n",
    "\n",
    "data = pd.read_csv('balanced_data.csv')\n",
    "tweets = data['tweet'].astype(str).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando embeddings...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47696/47696 [01:41<00:00, 467.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generating the embeddings\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for tweet in tqdm(tweets, desc = \"Generando embeddings...\"):\n",
    "    inputs = tokenizer(tweet, padding='max_length', max_length=32, truncation = True, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    embeddings.append(outputs.last_hidden_state.squeeze(0))\n",
    "\n",
    "embeddings_tensor = torch.stack(embeddings).cpu().numpy()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the embeddings\n",
    "\n",
    "np.save('embeddings_data_sequencial.npy', embeddings_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Embeddings Contextuales BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init_empty_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m      4\u001b[0m tokenizer_roberta \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> 5\u001b[0m model_roberta \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m model_roberta\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m )\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:4333\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4330\u001b[0m config\u001b[38;5;241m.\u001b[39mname_or_path \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m   4332\u001b[0m \u001b[38;5;66;03m# Instantiate model.\u001b[39;00m\n\u001b[0;32m-> 4333\u001b[0m model_init_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_init_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_is_ds_init_called\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4335\u001b[0m config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[1;32m   4336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_attn_implementation_autoset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3736\u001b[0m, in \u001b[0;36mPreTrainedModel.get_init_context\u001b[0;34m(cls, is_quantized, _is_ds_init_called)\u001b[0m\n\u001b[1;32m   3734\u001b[0m         init_contexts\u001b[38;5;241m.\u001b[39mappend(set_quantized_state())\n\u001b[1;32m   3735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3736\u001b[0m     init_contexts \u001b[38;5;241m=\u001b[39m [no_init_weights(), \u001b[43minit_empty_weights\u001b[49m()]\n\u001b[1;32m   3738\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m init_contexts\n",
      "\u001b[0;31mNameError\u001b[0m: name 'init_empty_weights' is not defined"
     ]
    }
   ],
   "source": [
    "model_name = 'cardiffnlp/twitter-roberta-base'\n",
    "max_len = 64\n",
    "\n",
    "tokenizer_roberta = AutoTokenizer.from_pretrained(model_name)\n",
    "model_roberta = AutoModel.from_pretrained(model_name)\n",
    "model_roberta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/balanced_data.csv')\n",
    "tweets = data['tweet'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(outputs, attention_mask):\n",
    "    token_embeddings = outputs.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "    sum_mask = input_mask_expanded.sum(1)\n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(texts, model, tokenizer, max_len=64, device=None):\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    embeddings_cls = []\n",
    "    embeddings_mean = []\n",
    "\n",
    "    for text in tqdm(texts, desc='Generando embeddings'):\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=max_len)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "        embeddings_cls.append(cls_embedding)\n",
    "\n",
    "        mean_embedding = mean_pooling(outputs, inputs['attention_mask']).squeeze().cpu().numpy()\n",
    "        embeddings_mean.append(mean_embedding)\n",
    "\n",
    "    return embeddings_cls, embeddings_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_cls, embeddings_mean = generate_embeddings(tweets, model_roberta, tokenizer_roberta, max_len)\n",
    "\n",
    "data[\"embedding_cls\"] = [vec.tolist() for vec in embeddings_cls]\n",
    "data[\"embedding_mean\"] = [vec.tolist() for vec in embeddings_mean]\n",
    "\n",
    "data.to_csv('data/embeddings_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/embeddings_data.csv\")\n",
    "df[\"embedding_mean\"] = df[\"embedding_mean\"].apply(ast.literal_eval)\n",
    "X = np.vstack(df[\"embedding_mean\"].values)\n",
    "labels = df[\"class\"].astype(str) if \"class\" in df.columns else None\n",
    "\n",
    "reducer = umap.UMAP(n_components=3, random_state=42)\n",
    "X_umap = reducer.fit_transform(X)\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    x=X_umap[:, 0],\n",
    "    y=X_umap[:, 1],\n",
    "    z=X_umap[:, 2],\n",
    "    color=labels,\n",
    "    title=\"UMAP 3D de embeddings de tweets\",\n",
    "    labels={\"color\": \"Clase\"}\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
