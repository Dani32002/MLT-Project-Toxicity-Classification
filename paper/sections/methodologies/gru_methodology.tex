\subsubsection{GRU Methodology}
\label{sec:gru}
For the GRU model, we begin with the use of pretrained embeddings where each sentence and sequence of words were represented as a fixed length word. Each input sample was structured as matrix of shapes, capturing the semantic composition of text. The sequences where then fit into the model where it captured temporal dependences and contextual information. Then the output was passed through fully connected layers with drop out regularization of 0.3. Finally a softmax activation was used to produce the predicted class probabilities. The model was trained, using a label dataset with binary cross-entropy and evaluated using accuracy F1-score and precision-recall metrics.